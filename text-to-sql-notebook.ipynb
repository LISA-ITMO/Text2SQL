{
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "ac71228f624f42b9ae142e8d0ae3c223": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdeb542fce2f4869a7ed2ad53c80fb23",
       "IPY_MODEL_84c7ecf90210445a8158ec8a6bd6fa92",
       "IPY_MODEL_7a923c7b3282478ab3b662f5f8babbf2"
      ],
      "layout": "IPY_MODEL_e063d9fa89b341a78d84e551e9cbaefc"
     }
    },
    "bdeb542fce2f4869a7ed2ad53c80fb23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9dcfb9a215b47378c1a82d804650f14",
      "placeholder": "​",
      "style": "IPY_MODEL_6b47028becbd49ab8bbf0245287567ce",
      "value": "Downloading readme: 100%"
     }
    },
    "84c7ecf90210445a8158ec8a6bd6fa92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dc7d9a79c6f4b66b93c3f41ccadde4d",
      "max": 95,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_960911afc6f940a2a18109a368ccb53e",
      "value": 95
     }
    },
    "7a923c7b3282478ab3b662f5f8babbf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be10ab64fe6145bcaac19bbf46b148f7",
      "placeholder": "​",
      "style": "IPY_MODEL_becb78efe3c5401da77b03363a18a54a",
      "value": " 95.0/95.0 [00:00&lt;00:00, 4.11kB/s]"
     }
    },
    "e063d9fa89b341a78d84e551e9cbaefc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9dcfb9a215b47378c1a82d804650f14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b47028becbd49ab8bbf0245287567ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1dc7d9a79c6f4b66b93c3f41ccadde4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "960911afc6f940a2a18109a368ccb53e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "be10ab64fe6145bcaac19bbf46b148f7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "becb78efe3c5401da77b03363a18a54a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a96dd3b74d924304a9c44825215b8668": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bc98b9710be4e57be624178d65aa2d4",
       "IPY_MODEL_9afa28f883f24918a0e91541930e70ce",
       "IPY_MODEL_08631ca611af41f09be087c47a5c6f72"
      ],
      "layout": "IPY_MODEL_f30c5a3085754df493e6dcfc8fa33b28"
     }
    },
    "7bc98b9710be4e57be624178d65aa2d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc82cfc4623b4900a60720509c8f6f34",
      "placeholder": "​",
      "style": "IPY_MODEL_a72c1a8c781748fdb7ac20e38329f500",
      "value": "Downloading data: 100%"
     }
    },
    "9afa28f883f24918a0e91541930e70ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_127b9083c3ca4e99aef1dcabc0d3e268",
      "max": 13514003,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_259fe22a2f7140d4a15574a267d8c8f6",
      "value": 13514003
     }
    },
    "08631ca611af41f09be087c47a5c6f72": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f24ebd97941240bc9b6aff74c6ac688a",
      "placeholder": "​",
      "style": "IPY_MODEL_5b721bfa7eac4e598f53bf254ba09994",
      "value": " 13.5M/13.5M [00:02&lt;00:00, 6.48MB/s]"
     }
    },
    "f30c5a3085754df493e6dcfc8fa33b28": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc82cfc4623b4900a60720509c8f6f34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a72c1a8c781748fdb7ac20e38329f500": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "127b9083c3ca4e99aef1dcabc0d3e268": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "259fe22a2f7140d4a15574a267d8c8f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f24ebd97941240bc9b6aff74c6ac688a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b721bfa7eac4e598f53bf254ba09994": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e839e21d4634493822f1672e84c2cc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61cef6c441b34c37a7df076cdf86e223",
       "IPY_MODEL_82798feeeedf42cf9124f424ab0e60bb",
       "IPY_MODEL_5936a5d3e9a34fb59e31ee1c78025bb1"
      ],
      "layout": "IPY_MODEL_6abb053b10bd48649366d84c506e273d"
     }
    },
    "61cef6c441b34c37a7df076cdf86e223": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b989886a13c40809cab893f60475b80",
      "placeholder": "​",
      "style": "IPY_MODEL_7c6c245feb274bd6a88e6738b37acbb9",
      "value": "Generating validation split: "
     }
    },
    "82798feeeedf42cf9124f424ab0e60bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc4b64d37d444e279c5177cb068df39d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eee873012c1b4e42aa28f50cd9530bdd",
      "value": 1
     }
    },
    "5936a5d3e9a34fb59e31ee1c78025bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03c6d313d405419c9a19594c5e87c0ba",
      "placeholder": "​",
      "style": "IPY_MODEL_5705a034d3a345f0aae19a6813b68af4",
      "value": " 49456/0 [00:00&lt;00:00, 217712.58 examples/s]"
     }
    },
    "6abb053b10bd48649366d84c506e273d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b989886a13c40809cab893f60475b80": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c6c245feb274bd6a88e6738b37acbb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc4b64d37d444e279c5177cb068df39d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "eee873012c1b4e42aa28f50cd9530bdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "03c6d313d405419c9a19594c5e87c0ba": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5705a034d3a345f0aae19a6813b68af4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3971bf66faeb4aeb9394b9aeaf746e04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11b0079e98564a1292159b11becabe91",
       "IPY_MODEL_9edce60d25c1480f886d74f5cc8fbfd1",
       "IPY_MODEL_7ba6a3468a14455eb2e2654c75bd08c9"
      ],
      "layout": "IPY_MODEL_9158b283aa3a4ce99bc7c270e18eb646"
     }
    },
    "11b0079e98564a1292159b11becabe91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bc3ac660c2b47948867217aea92e209",
      "placeholder": "​",
      "style": "IPY_MODEL_44f19b960b2f400b81277c156ef0e82d",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "9edce60d25c1480f886d74f5cc8fbfd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2394fc952f6b4d37be090aca985860e9",
      "max": 6961671585,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2a2465083684d19856656f15f5f62ef",
      "value": 6961671585
     }
    },
    "7ba6a3468a14455eb2e2654c75bd08c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5087bcf0cb3a433aa7e4979e6b8c37c3",
      "placeholder": "​",
      "style": "IPY_MODEL_4b732776189f4ba788cbd285204c14d0",
      "value": " 6.96G/6.96G [01:08&lt;00:00, 104MB/s]"
     }
    },
    "9158b283aa3a4ce99bc7c270e18eb646": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bc3ac660c2b47948867217aea92e209": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44f19b960b2f400b81277c156ef0e82d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2394fc952f6b4d37be090aca985860e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a2465083684d19856656f15f5f62ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5087bcf0cb3a433aa7e4979e6b8c37c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b732776189f4ba788cbd285204c14d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7437329,
     "sourceType": "datasetVersion",
     "datasetId": 4328597
    }
   ],
   "dockerImageVersionId": 30635,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers pynvml datasets accelerate torch==2.0.1 --quiet"
   ],
   "metadata": {
    "id": "OOk8NpIxCP8s",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8e783431-1128-49cd-bab1-563602910ff4",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:41:37.643688Z",
     "iopub.execute_input": "2024-01-20T19:41:37.644218Z",
     "iopub.status.idle": "2024-01-20T19:44:06.108213Z",
     "shell.execute_reply.started": "2024-01-20T19:41:37.644187Z",
     "shell.execute_reply": "2024-01-20T19:44:06.107028Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import Optional\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import HfArgumentParser\n",
    "from pynvml import *"
   ],
   "metadata": {
    "id": "Zzs8k5I8MGmW",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:44:06.110241Z",
     "iopub.execute_input": "2024-01-20T19:44:06.110598Z",
     "iopub.status.idle": "2024-01-20T19:44:21.776046Z",
     "shell.execute_reply.started": "2024-01-20T19:44:06.110570Z",
     "shell.execute_reply": "2024-01-20T19:44:21.775223Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "templates = []\n",
    "with open('/kaggle/input/rusql-to-text/dataset.csv', 'r', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    next(spamreader)\n",
    "    for row in spamreader:\n",
    "        templates.append({\n",
    "            \"question\": row[1].strip(),\n",
    "            \"context\": row[2].strip(),\n",
    "            \"answer\": row[3].strip(),\n",
    "        })"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-21T10:46:47.588163Z",
     "iopub.execute_input": "2024-01-21T10:46:47.588479Z",
     "iopub.status.idle": "2024-01-21T10:46:47.697347Z",
     "shell.execute_reply.started": "2024-01-21T10:46:47.588456Z",
     "shell.execute_reply": "2024-01-21T10:46:47.696673Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = []\n",
    "for i in templates:\n",
    "    context = \"\"\n",
    "    for t in i[\"context\"].strip().split(\"CREATE\")[1:]:\n",
    "        table_name = t.split('(\\n')[0][7:].strip()\n",
    "        if '.' in table_name:\n",
    "            table_name = table_name.split('.')[1]\n",
    "        if table_name in i['answer']:\n",
    "            context += \"CREATE\" + t\n",
    "            break\n",
    "    data.append({\n",
    "            \"question\": i[\"question\"],\n",
    "            \"context\": context,\n",
    "            \"answer\": i[\"answer\"],\n",
    "        })"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-21T10:46:47.700529Z",
     "iopub.execute_input": "2024-01-21T10:46:47.701484Z",
     "iopub.status.idle": "2024-01-21T10:46:47.711018Z",
     "shell.execute_reply.started": "2024-01-21T10:46:47.701447Z",
     "shell.execute_reply": "2024-01-21T10:46:47.709993Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data[0]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-21T10:46:52.434887Z",
     "iopub.execute_input": "2024-01-21T10:46:52.435203Z",
     "iopub.status.idle": "2024-01-21T10:46:52.441967Z",
     "shell.execute_reply.started": "2024-01-21T10:46:52.435180Z",
     "shell.execute_reply": "2024-01-21T10:46:52.441178Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "execution_count": 3,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'question': 'Вывести всю информацию об игровых боссах.',\n 'context': 'CREATE TABLE terraria.boss (\\n  id_boss INT(255) NOT NULL PRIMARY KEY,\\n  damage INT(255) NOT NULL,\\n  level INT(255) NOT NULL,\\n  magic_trick VARCHAR(255) NOT NULL,\\n  name VARCHAR(255) NOT NULL,\\n  speed INT(255) NOT NULL,\\n);\\n\\n',\n 'answer': 'SELECT * FROM boss;'}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in data:\n",
    "    if i['context'] == \"\":\n",
    "        print(i)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-20T19:44:21.877078Z",
     "iopub.execute_input": "2024-01-20T19:44:21.877380Z",
     "iopub.status.idle": "2024-01-20T19:44:21.884115Z",
     "shell.execute_reply.started": "2024-01-20T19:44:21.877357Z",
     "shell.execute_reply": "2024-01-20T19:44:21.883405Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, Features, Value\n",
    "\n",
    "\n",
    "# with open(r\"/kaggle/input/text-to-sql/know_sql_val3.json\") as f:\n",
    "#     templates = json.load(f)\n",
    "    \n",
    "\n",
    "dictionary = {\n",
    "    \"train\": data\n",
    "}\n",
    "\n",
    "features = Features({\n",
    "    'train': {\n",
    "        \"question\": Value('string'),\n",
    "        \"context\": Value('string'),\n",
    "        \"answer\": Value('string'),\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict(dictionary, features=features)\n",
    "dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-20T19:44:21.885393Z",
     "iopub.execute_input": "2024-01-20T19:44:21.885707Z",
     "iopub.status.idle": "2024-01-20T19:44:21.954194Z",
     "shell.execute_reply.started": "2024-01-20T19:44:21.885683Z",
     "shell.execute_reply": "2024-01-20T19:44:21.953350Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset({\n    features: ['train'],\n    num_rows: 757\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def load_samples(dataset_path, tokenizer):\n",
    "    samples = []\n",
    "    #with open(dataset_path, 'r') as f:\n",
    "    for sample in tqdm.tqdm(dataset['train']):\n",
    "        try:\n",
    "            seed = '<SC6>' + \"По таблице:\\n\" + sample['context'] + \"\\nВыполни задание:\" + sample['question'] + '<extra_id_0>'\n",
    "            reply = '<extra_id_0>' + sample['answer']\n",
    "            input_tokens = tokenizer.encode(seed, add_special_tokens=False, truncation=True, max_length=1024)\n",
    "            output_tokens = tokenizer.encode(reply, add_special_tokens=False)\n",
    "            if len(input_tokens) < 128 and len(output_tokens) < 768:\n",
    "                samples.append({'input_tokens': input_tokens, 'output_tokens': output_tokens})\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "    return samples"
   ],
   "metadata": {
    "id": "_axABYiiCNVc",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:44:21.955516Z",
     "iopub.execute_input": "2024-01-20T19:44:21.955822Z",
     "iopub.status.idle": "2024-01-20T19:44:21.962777Z",
     "shell.execute_reply.started": "2024-01-20T19:44:21.955798Z",
     "shell.execute_reply": "2024-01-20T19:44:21.961910Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, dataset):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = 0\n",
    "        self.max_output_len = 0\n",
    "        self.samples = []\n",
    "        self._data = dataset._data\n",
    "        self._fingerprint = dataset._fingerprint\n",
    "        self._info = dataset._info\n",
    "\n",
    "        self.bos_token_id = tokenizer.encode('<s>', add_special_tokens=False)[0]\n",
    "        self.eos_token_id = tokenizer.encode('</s>', add_special_tokens=False)[0]\n",
    "        self.pad_token_id = tokenizer.encode('<pad>', add_special_tokens=False)[0]\n",
    "\n",
    "        for sample in samples:\n",
    "            input_ids = sample['input_tokens']\n",
    "            output_ids = sample['output_tokens'] + [self.eos_token_id]\n",
    "            self.samples.append((input_ids, output_ids))\n",
    "            self.max_input_len = max(self.max_input_len, len(input_ids))\n",
    "            self.max_output_len = max(self.max_output_len, len(output_ids))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        input_ids, output_ids = self.samples[index]\n",
    "\n",
    "        input_npad = self.max_input_len - len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids) + [0] * input_npad\n",
    "        input_ids = input_ids + input_npad * [self.pad_token_id]\n",
    "\n",
    "        output_npad = self.max_output_len - len(output_ids)\n",
    "        labels = output_ids + output_npad * [-100]\n",
    "\n",
    "        return {'input_ids': torch.LongTensor(input_ids), 'attention_mask': attention_mask,\n",
    "                'labels': torch.LongTensor(labels)}"
   ],
   "metadata": {
    "id": "xKujlt5WDM5n",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:44:21.963754Z",
     "iopub.execute_input": "2024-01-20T19:44:21.964059Z",
     "iopub.status.idle": "2024-01-20T19:44:21.976016Z",
     "shell.execute_reply.started": "2024-01-20T19:44:21.964035Z",
     "shell.execute_reply": "2024-01-20T19:44:21.975146Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pretrained_model_name = \"ai-forever/FRED-T5-large\""
   ],
   "metadata": {
    "id": "HaZ4YkSdDOot",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:54:13.743111Z",
     "iopub.execute_input": "2024-01-20T19:54:13.743952Z",
     "iopub.status.idle": "2024-01-20T19:54:13.749601Z",
     "shell.execute_reply.started": "2024-01-20T19:54:13.743912Z",
     "shell.execute_reply": "2024-01-20T19:54:13.748455Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = transformers.T5ForConditionalGeneration.from_pretrained(pretrained_model_name)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "3971bf66faeb4aeb9394b9aeaf746e04",
      "11b0079e98564a1292159b11becabe91",
      "9edce60d25c1480f886d74f5cc8fbfd1",
      "7ba6a3468a14455eb2e2654c75bd08c9",
      "9158b283aa3a4ce99bc7c270e18eb646",
      "7bc3ac660c2b47948867217aea92e209",
      "44f19b960b2f400b81277c156ef0e82d",
      "2394fc952f6b4d37be090aca985860e9",
      "f2a2465083684d19856656f15f5f62ef",
      "5087bcf0cb3a433aa7e4979e6b8c37c3",
      "4b732776189f4ba788cbd285204c14d0"
     ]
    },
    "id": "Ij3BdxLaDQju",
    "outputId": "0cb18b73-06f8-4142-e2f7-ad2bf7718388",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:54:16.531924Z",
     "iopub.execute_input": "2024-01-20T19:54:16.532525Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13d7212d924e440b8ebe99f0167f5644"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2d48c0f130b41d3953a29af08bc0127"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e112c78aeb59446f8093c6a2ce7efa2f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2194284513b740beb6e6018782f76a32"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1575285aa4541e5a878100cdb2d9df8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You are using a model of type gpt2 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/3.14G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3443b713551142f1937a9bc2b9acf4a5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at ai-forever/rugpt3large_based_on_gpt2 and are newly initialized: ['transformer.encoder.block.0.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.v.weight', 'transformer.encoder.block.3.layer.0.layer_norm.weight', 'transformer.encoder.block.1.layer.1.layer_norm.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'transformer.encoder.block.5.layer.1.layer_norm.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.k.weight', 'transformer.lm_head.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.3.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.5.layer.2.DenseReluDense.wo.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.1.layer.1.EncDecAttention.q.weight', 'transformer.decoder.block.3.layer.1.layer_norm.weight', 'transformer.decoder.block.1.layer.1.EncDecAttention.o.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.q.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.4.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.4.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.4.layer.0.layer_norm.weight', 'transformer.decoder.block.3.layer.2.DenseReluDense.wo.weight', 'transformer.encoder.block.2.layer.0.layer_norm.weight', 'transformer.decoder.block.2.layer.0.layer_norm.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.3.layer.1.layer_norm.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.v.weight', 'transformer.decoder.block.5.layer.2.DenseReluDense.wi.weight', 'transformer.encoder.block.2.layer.1.DenseReluDense.wo.weight', 'transformer.encoder.block.5.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.v.weight', 'transformer.decoder.block.1.layer.1.EncDecAttention.v.weight', 'transformer.decoder.block.3.layer.2.DenseReluDense.wi.weight', 'transformer.encoder.block.0.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.5.layer.0.layer_norm.weight', 'transformer.encoder.embed_tokens.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.5.layer.2.layer_norm.weight', 'transformer.decoder.block.0.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.4.layer.2.DenseReluDense.wo.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.o.weight', 'transformer.encoder.block.2.layer.1.layer_norm.weight', 'transformer.encoder.block.0.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.0.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.2.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.v.weight', 'transformer.encoder.final_layer_norm.weight', 'transformer.decoder.block.1.layer.1.EncDecAttention.k.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.1.layer.2.layer_norm.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.k.weight', 'transformer.shared.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.1.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.5.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.0.layer.1.layer_norm.weight', 'transformer.decoder.block.0.layer.2.layer_norm.weight', 'transformer.encoder.block.1.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.v.weight', 'transformer.encoder.block.0.layer.0.layer_norm.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.5.layer.1.DenseReluDense.wi.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.q.weight', 'transformer.decoder.block.4.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.1.layer.1.layer_norm.weight', 'transformer.decoder.block.1.layer.0.layer_norm.weight', 'transformer.encoder.block.3.layer.0.SelfAttention.q.weight', 'transformer.encoder.block.3.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.5.layer.1.layer_norm.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.0.layer.0.SelfAttention.o.weight', 'transformer.decoder.block.4.layer.1.layer_norm.weight', 'transformer.decoder.block.3.layer.2.layer_norm.weight', 'transformer.encoder.block.4.layer.1.DenseReluDense.wo.weight', 'transformer.decoder.block.2.layer.2.DenseReluDense.wi.weight', 'transformer.decoder.final_layer_norm.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.o.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.v.weight', 'transformer.decoder.embed_tokens.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.2.layer.1.EncDecAttention.q.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.k.weight', 'transformer.encoder.block.0.layer.1.layer_norm.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.1.layer.2.DenseReluDense.wo.weight', 'transformer.encoder.block.4.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.o.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.5.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.3.layer.0.SelfAttention.k.weight', 'transformer.encoder.block.4.layer.0.layer_norm.weight', 'transformer.encoder.block.1.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.3.layer.0.layer_norm.weight', 'transformer.encoder.block.1.layer.0.layer_norm.weight', 'transformer.decoder.block.4.layer.1.EncDecAttention.o.weight', 'transformer.decoder.block.0.layer.2.DenseReluDense.wo.weight', 'transformer.decoder.block.0.layer.1.EncDecAttention.q.weight', 'transformer.decoder.block.2.layer.2.layer_norm.weight', 'transformer.decoder.block.2.layer.2.DenseReluDense.wo.weight', 'transformer.encoder.block.4.layer.1.layer_norm.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.k.weight', 'transformer.decoder.block.0.layer.0.layer_norm.weight', 'transformer.encoder.block.2.layer.0.SelfAttention.k.weight', 'transformer.decoder.block.4.layer.2.layer_norm.weight', 'transformer.decoder.block.1.layer.0.SelfAttention.v.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.v.weight', 'transformer.encoder.block.5.layer.0.layer_norm.weight', 'transformer.decoder.block.2.layer.0.SelfAttention.q.weight', 'transformer.decoder.block.5.layer.1.EncDecAttention.v.weight', 'transformer.decoder.block.2.layer.1.layer_norm.weight', 'transformer.decoder.block.3.layer.1.EncDecAttention.q.weight', 'transformer.encoder.block.1.layer.1.DenseReluDense.wo.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "\n",
    "tokenizer.add_special_tokens({'bos_token': '<s>', 'eos_token': '</s>', 'pad_token': '<pad>'})"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_4EGPmmEU7N",
    "outputId": "f99ca15c-c2a7-4f6c-d5cd-5315bee725de",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:52:34.331319Z",
     "iopub.execute_input": "2024-01-20T19:52:34.331611Z",
     "iopub.status.idle": "2024-01-20T19:52:34.388932Z",
     "shell.execute_reply.started": "2024-01-20T19:52:34.331586Z",
     "shell.execute_reply": "2024-01-20T19:52:34.388020Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_samples = load_samples(dataset, tokenizer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhsF7JtPEYVN",
    "outputId": "fc396802-4e86-4bda-eed8-5cedc2f0f687",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:52:34.390050Z",
     "iopub.execute_input": "2024-01-20T19:52:34.390338Z",
     "iopub.status.idle": "2024-01-20T19:52:34.903145Z",
     "shell.execute_reply.started": "2024-01-20T19:52:34.390306Z",
     "shell.execute_reply": "2024-01-20T19:52:34.902275Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 757/757 [00:00<00:00, 1536.67it/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_samples)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-20T19:52:34.904232Z",
     "iopub.execute_input": "2024-01-20T19:52:34.904545Z",
     "iopub.status.idle": "2024-01-20T19:52:34.910476Z",
     "shell.execute_reply.started": "2024-01-20T19:52:34.904521Z",
     "shell.execute_reply": "2024-01-20T19:52:34.909568Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "56"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NMb-h9yHOEW",
    "outputId": "fe3bcdac-7a2c-438b-abf0-3762e3c5cde9",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:52:34.911535Z",
     "iopub.execute_input": "2024-01-20T19:52:34.911828Z",
     "iopub.status.idle": "2024-01-20T19:52:34.920028Z",
     "shell.execute_reply.started": "2024-01-20T19:52:34.911792Z",
     "shell.execute_reply": "2024-01-20T19:52:34.919137Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "0.25.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=1, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    #warmup_steps=250,# number of warmup steps for learning rate scheduler\n",
    "    learning_rate = 5e-5,\n",
    "    gradient_accumulation_steps=35,\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type = \"constant\",\n",
    "#   torch_compile=True,\n",
    "    save_steps=1500,\n",
    "    save_total_limit = 1\n",
    "    )\n",
    "\n",
    "train_dataset = SFTDataset(train_samples, tokenizer, dataset)\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=None,\n",
    "        optimizers = (torch.optim.AdamW(model.parameters(),lr=5e-5),None)\n",
    "    )"
   ],
   "metadata": {
    "id": "nUBYPnTOL0kH",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:52:34.921089Z",
     "iopub.execute_input": "2024-01-20T19:52:34.921417Z",
     "iopub.status.idle": "2024-01-20T19:52:41.193752Z",
     "shell.execute_reply.started": "2024-01-20T19:52:34.921393Z",
     "shell.execute_reply": "2024-01-20T19:52:41.192730Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "train_result = trainer.train()"
   ],
   "metadata": {
    "id": "CR_B2Wbncoch",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "outputId": "7a3373d8-6112-4faa-af96-986415edca72",
    "execution": {
     "iopub.status.busy": "2024-01-20T19:52:54.338903Z",
     "iopub.execute_input": "2024-01-20T19:52:54.339691Z",
     "iopub.status.idle": "2024-01-20T19:53:40.414642Z",
     "shell.execute_reply.started": "2024-01-20T19:52:54.339658Z",
     "shell.execute_reply": "2024-01-20T19:53:40.413715Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
     "output_type": "stream"
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": "  ········································\n"
    },
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240120_195303-fnrzq2kf</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/yuraz-team/huggingface/runs/fnrzq2kf' target=\"_blank\">cerulean-smoke-14</a></strong> to <a href='https://wandb.ai/yuraz-team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/yuraz-team/huggingface' target=\"_blank\">https://wandb.ai/yuraz-team/huggingface</a>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/yuraz-team/huggingface/runs/fnrzq2kf' target=\"_blank\">https://wandb.ai/yuraz-team/huggingface/runs/fnrzq2kf</a>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:1\u001B[0m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1537\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1535\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1537\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1538\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1539\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1541\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1854\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1851\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1853\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1854\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1856\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1857\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1859\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1860\u001B[0m ):\n\u001B[1;32m   1861\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1862\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2735\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2732\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2734\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2735\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2738\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2758\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2756\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2757\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2758\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2759\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2760\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2761\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:680\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 680\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:668\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 668\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:14\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[0;32m---> 14\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1743\u001B[0m, in \u001B[0;36mT5ForConditionalGeneration.forward\u001B[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1740\u001B[0m         decoder_attention_mask \u001B[38;5;241m=\u001B[39m decoder_attention_mask\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mfirst_device)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[0;32m-> 1743\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1745\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1746\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1747\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1748\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1749\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1750\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1751\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1752\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1753\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1754\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1755\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1756\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1758\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m decoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1110\u001B[0m, in \u001B[0;36mT5Stack.forward\u001B[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1095\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m   1096\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39mforward,\n\u001B[1;32m   1097\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1107\u001B[0m         output_attentions,\n\u001B[1;32m   1108\u001B[0m     )\n\u001B[1;32m   1109\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1110\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1114\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1120\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1121\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1122\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1124\u001B[0m \u001B[38;5;66;03m# layer_outputs is a tuple with:\u001B[39;00m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001B[39;00m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:694\u001B[0m, in \u001B[0;36mT5Block.forward\u001B[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001B[0m\n\u001B[1;32m    691\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    692\u001B[0m     self_attn_past_key_value, cross_attn_past_key_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 694\u001B[0m self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    695\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    696\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    700\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    703\u001B[0m hidden_states, present_key_value_state \u001B[38;5;241m=\u001B[39m self_attention_outputs[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    704\u001B[0m attention_outputs \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m2\u001B[39m:]  \u001B[38;5;66;03m# Keep self-attention outputs and relative position weights\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:601\u001B[0m, in \u001B[0;36mT5LayerSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    590\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    591\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    592\u001B[0m     hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    598\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    599\u001B[0m ):\n\u001B[1;32m    600\u001B[0m     normed_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(hidden_states)\n\u001B[0;32m--> 601\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSelfAttention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnormed_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    610\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(attention_output[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    611\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (hidden_states,) \u001B[38;5;241m+\u001B[39m attention_output[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:520\u001B[0m, in \u001B[0;36mT5Attention.forward\u001B[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# get query states\u001B[39;00m\n\u001B[0;32m--> 520\u001B[0m query_states \u001B[38;5;241m=\u001B[39m shape(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001B[39;00m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;66;03m# get key/value states\u001B[39;00m\n\u001B[1;32m    523\u001B[0m key_states \u001B[38;5;241m=\u001B[39m project(\n\u001B[1;32m    524\u001B[0m     hidden_states, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk, key_value_states, past_key_value[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    525\u001B[0m )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.89 GiB total capacity; 15.06 GiB already allocated; 24.12 MiB free; 15.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ],
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.89 GiB total capacity; 15.06 GiB already allocated; 24.12 MiB free; 15.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train3\", metrics)\n",
    "trainer.save_metrics(\"train3\", metrics)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-20T18:09:12.541367Z",
     "iopub.execute_input": "2024-01-20T18:09:12.542127Z",
     "iopub.status.idle": "2024-01-20T18:09:12.550869Z",
     "shell.execute_reply.started": "2024-01-20T18:09:12.542095Z",
     "shell.execute_reply": "2024-01-20T18:09:12.549898Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": "***** train3 metrics *****\n  epoch                    =        5.0\n  total_flos               =  4226552GF\n  train_loss               =        0.0\n  train_runtime            = 0:16:50.76\n  train_samples_per_second =      1.904\n  train_steps_per_second   =      0.054\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "U_hoLeoDvDS3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "metadata": {
    "id": "0C48xR5Nc7rA",
    "execution": {
     "iopub.status.busy": "2024-01-20T18:09:26.287980Z",
     "iopub.execute_input": "2024-01-20T18:09:26.288726Z",
     "iopub.status.idle": "2024-01-20T18:09:26.315920Z",
     "shell.execute_reply.started": "2024-01-20T18:09:26.288695Z",
     "shell.execute_reply": "2024-01-20T18:09:26.314886Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11009e5610174d0e9329fed194c7336d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer.push_to_hub(\"yuraz28/FRED-T5-know_sql-test5\")\n",
    "model.push_to_hub(\"yuraz28/FRED-T5-know_sql-test5\")"
   ],
   "metadata": {
    "id": "23YWRtcIdGqn",
    "execution": {
     "iopub.status.busy": "2024-01-20T18:10:03.484453Z",
     "iopub.execute_input": "2024-01-20T18:10:03.485206Z",
     "iopub.status.idle": "2024-01-20T18:11:38.174920Z",
     "shell.execute_reply.started": "2024-01-20T18:10:03.485174Z",
     "shell.execute_reply": "2024-01-20T18:11:38.173727Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/3.28G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31eebb9412154644b0ee749895661704"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 24,
     "output_type": "execute_result",
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/yuraz28/FRED-T5-know_sql-test5/commit/29cdbcd4b777f36f00ecf655c3d8dea6f21bc26d', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='29cdbcd4b777f36f00ecf655c3d8dea6f21bc26d', pr_url=None, pr_revision=None, pr_num=None)"
     },
     "metadata": {}
    }
   ]
  }
 ]
}
